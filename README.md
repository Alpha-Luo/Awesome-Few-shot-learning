# Awesome-Few-shot-learning

This repo provides an up-to-date list of progress made in Few-shot Learning, which includes but not limited to papers, datasets, codebases and etc. 

## Papers

 - ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond. [[PDF](https://arxiv.org/pdf/2202.10108.pdf)] [[Code](https://github.com/Annbless/ViTAE)] [[Covered Media](https://mp.weixin.qq.com/s/hQVv0UTT1C6M0r3HHQx1HQ)] `arXiv` `2022`
 - The Effects of Regularization and Data Augmentation are Class Dependent. [[PDF](https://arxiv.org/pdf/2204.03632.pdf)] [[Covered Media](https://www.datalearner.com/blog/1051649688738359)] `arXiv` `2022`
 - Designing Effective Sparse Expert Models. [[PDF](https://arxiv.org/pdf/2202.08906.pdf)] [[Covered Media](https://www.toutiao.com/article/7073025419217338894
)] `arXiv` `2022`
 - Hardware Approximate Techniques for Deep Neural Network Accelerators: A Survey. [[PDF](https://arxiv.org/pdf/2203.08737v1.pdf)] [[Covered Media](https://www.toutiao.com/article/7076617388380193292)] `ACM Computing Surveys (CSUR)` `2022`
 - Sub-bit Neural Networks: Learning to Compress and Accelerate Binary Neural Networks. [[PDF](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Sub-Bit_Neural_Networks_Learning_To_Compress_and_Accelerate_Binary_Neural_ICCV_2021_paper.pdf)] [[Code](https://github.com/yikaiw/SNN)] `ICCV` `2021`
 - Training Compute-Optimal Large Language Models. [[PDF](https://arxiv.org/pdf/2203.15556.pdf)] [[Covered Media](https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models)] `arXiv` `2022` 
 - PaLM: Scaling Language Modeling with Pathways. [[PDF](https://www.datalearner.com/resources/papers/PaLM-paper.pdf)] [[Covered Media](https://www.sohu.com/a/535823925_629135)] `arXiv` `2022`
 - GAIA: A Transfer Learning System of Object Detection that Fits Your Needs. [[PDF](https://arxiv.org/pdf/2106.11346.pdf)] [[Code](https://github.com/GAIA-vision
)] [[Covered Media](https://mp.weixin.qq.com/s/hz-e5aE2o3SU3btGZjVAyw)] `CVPR` `2021`
 - Scaling Vision with Sparse Mixture of Experts. [[PDF](https://arxiv.org/pdf/2106.05974.pdf)] [[Code](https://github.com/google-research/vmoe)] [[Covered Media](https://mp.weixin.qq.com/s/_EAmHwqNYQmoM4AS-Lqmxg)] `NeurIPS` `2021`
 - Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models. [[PDF](https://arxiv.org/pdf/2203.06904.pdf)] [[Code](https://github.com/thunlp/OpenDelta)] [[Covered Media](https://mp.weixin.qq.com/s/1F24Px2d86LZ9MIdP-m_sA)] `arXiv` `2022`
 - Tensor Programs V:Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. [[PDF](https://arxiv.org/pdf/2203.03466v1.pdf)] [[Code](https://github.com/microsoft/mup)] [[Covered Media](https://mp.weixin.qq.com/s/CyR_0vynOOzS8v6NeYSptg)] `NeurIPS` `2021`
 - MetaFormer is Actually What You Need for Vision. [[PDF](https://arxiv.org/pdf/2111.11418.pdf)] [[Code](https://github.com/sail-sg/poolformer)] [[Covered Media](https://zhuanlan.zhihu.com/p/437210116)] `arXiv` `2021`
 - The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training. [[PDF](https://openreview.net/forum?id=VBZJ_3tz-t)] [[Code](https://github.com/VITA-Group/Random_Pruning)] [[Covered Media](https://www.toutiao.com/article/7085150891724259871)] `ICLR` `2022`
 - TokenLearner: What Can 8 Learned Tokens Do for Images and Videos? [[PDF](https://arxiv.org/pdf/2106.11297.pdf)] [[Code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner)] [[Covered Media](https://www.toutiao.com/article/7041005282473869837)] `NeurIPS` `2021`
 - Decoupled Knowledge Distillation [[PDF](https://arxiv.org/pdf/2203.08679.pdf)] [[Code](https://github.com/megvii-research/mdistiller)] [[Covered Media](https://www.toutiao.com/article/7086633860603200037)] `arXiv` `2022`


## Frameworks

 - [OpenGVLab开源平台](https://opengvlab.shlab.org.cn/)：上海人工智能实验室联合商汤科技、香港中文大学、上海交通大学共同发布通用视觉开源平台OpenGVLab，面向学术界和产业界开放其超高效预训练模型，和千万级精标注、十万级标签量的公开数据集，为全球开发者提升各类下游视觉任务模型训练提供重要支持。... [[更多讨论](https://www.toutiao.com/article/7068489165679591949)]
 - [欢聚时代AI跨平台推理框架VNN](https://github.com/joyycom/VNN)：欢聚集团自研的深度学习前向推理框架VNN，具有高性能、低功耗、多平台、轻量级的特性，可用于移动端、PC端以及服务端神经网络模型的部署，并在实际业务产品中加以打磨优化。目前VNN框架已经能很好的支持直播、短视频等应用场景。... [[更多讨论](https://www.toutiao.com/article/7042648326642090535)]
 - [字节跳动大模型训练框架 veGiantModel](https://github.com/volcengine/veGiantModel):字节跳动 AML 团队内部开发了火山引擎大模型训练框架 veGiantModel，比 Megatron 和 DeepSpeed 更快。... [[更多讨论](https://mp.weixin.qq.com/s/N6jZYh8YLGzjo-gCqtz6XQ)]
 - [中科院“紫东太初”三模态大模型](https://gitee.com/zidongtaichu/multi-modal-models)：中科院自动化所以全栈国产化基础软硬件昇腾AI平台为基础，依托武汉人工智能计算中心，研发了面向超大规模的高效分布式训练框架，在图、文、音三个基础模型上加入跨模态编码和解码网络，基于昇思MindSpore框架，打造了业内首个千亿参数三模态大模型“紫东太初”。... [[更多讨论](http://ia.cas.cn/xwzx/kydt/202109/t20210927_6215538.html)]
 - [Colossal-AI](https://github.com/hpcaitech/ColossalAI)：Colossal-AI 提供了一系列并行训练组件，目标是让分布式 AI 模型训练像普通的单 GPU 模型一样简单，并提供的友好工具可以在几行代码内快速开始分布式训练。... [[Docs](https://colossalai.org/zh-Hans/docs/concepts/colossalai_overview/)]

## Discussions

 - [清华刘知远：大模型「十问」，寻找新范式下的研究方向](https://www.toutiao.com/article/7083376984138596877)：大模型的出现迎来了AI研究的新时代，其所带来的结果提升十分显著，超越了很多领域中针对研究问题设计特定算法实现的提升。那么，在大模型时代有哪些新问题亟待关注和探索？...
 - [2022前展望大模型的未来，周志华、唐杰、杨红霞这些大咖怎么看？](https://mp.weixin.qq.com/s/RqkQzeR5BOVpU7tj_zUgqQ)：2021 是大模型爆发之年，我们见证了大模型的惊艳，但也了解了目前大模型的一些局限，如显著的高能耗等问题。大模型接下来会如何发展？岁末年初之际，让我们回顾大模型的过去，展望大模型的未来。...
